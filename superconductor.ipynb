{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "setattr(np, \"int\", int)\n",
    "setattr(np, \"float\", float)\n",
    "setattr(np, \"bool\", bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import functools as ft\n",
    "from typing import List\n",
    "\n",
    "import design_bench\n",
    "import diffrax as dfx\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from jaxtyping import Array, Float, Integer, PRNGKeyArray, PyTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from diffusionlib.sampler import DDIMVP\n",
    "from diffusionlib.optimizer import SMCDiffOptOptimizer\n",
    "\n",
    "import optax  # manually fix some modules that use tuple type-hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Config\n",
    "T = jnp.array(1000)\n",
    "BETA_MIN = jnp.array(0.1) / T\n",
    "BETA_MAX = jnp.array(20) / T\n",
    "\n",
    "LEARNING_RATE = 1e3\n",
    "\n",
    "key = random.PRNGKey(100)\n",
    "task = design_bench.make(\"Superconductor-RandomForest-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "# NOTE: we don't use the y for training; just want associated correctly\n",
    "train_x, val_x, train_y, val_y = train_test_split(task.x, task.y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the diffusion\n",
    "def beta_t(\n",
    "    t: Float[Array, \" batch\"],\n",
    "    beta_min: Float[Array, \"\"] = BETA_MIN,\n",
    "    beta_max: Float[Array, \"\"] = BETA_MAX,\n",
    ") -> Float[Array, \" batch\"]:\n",
    "    return beta_min + t * (beta_max - beta_min) / T\n",
    "\n",
    "\n",
    "def alpha_t(t: Float[Array, \" batch\"]) -> Float[Array, \" batch\"]:\n",
    "    return 1 - beta_t(t)\n",
    "\n",
    "\n",
    "alpha = alpha_t(jnp.arange(T + 1))\n",
    "cumulative_alpha_values = jnp.cumprod(alpha)\n",
    "\n",
    "def c_t(t: Integer[Array, \" batch\"]) -> Float[Array, \" batch\"]:\n",
    "    return jnp.sqrt(cumulative_alpha_values[t])\n",
    "\n",
    "\n",
    "def d_t(t: Integer[Array, \" batch\"]) -> Float[Array, \" batch\"]:\n",
    "    return jnp.sqrt(1 - cumulative_alpha_values[t])\n",
    "\n",
    "def forward_marginal(\n",
    "    key: PRNGKeyArray, x_0: Float[Array, \"batch dim\"], t: Integer[Array, \" batch\"]\n",
    ") -> Float[Array, \"batch dim\"]:\n",
    "    return c_t(t) * x_0 + d_t(t) ** 2 * random.normal(key, x_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network, loss, and training loop\n",
    "\n",
    "\n",
    "class FullyConnectedWithTime(eqx.Module):\n",
    "    \"\"\"A simple model with multiple fully connected layers and some fourier features for the time\n",
    "    variable.\n",
    "    \"\"\"\n",
    "\n",
    "    layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(self, in_size: int, key: PRNGKeyArray):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        out_size = in_size\n",
    "\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(in_size + 4, 256, key=key1),\n",
    "            eqx.nn.Linear(256, 256, key=key2),\n",
    "            eqx.nn.Linear(256, 256, key=key3),\n",
    "            eqx.nn.Linear(256, out_size, key=key4),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Array, t: Array) -> Array:\n",
    "        t_fourier = jnp.array(\n",
    "            [t - 0.5, jnp.cos(2 * jnp.pi * t), jnp.sin(2 * jnp.pi * t), -jnp.cos(4 * jnp.pi * t)],\n",
    "        )\n",
    "\n",
    "        x = jnp.concatenate([x, t_fourier])\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = jax.nn.relu(layer(x))\n",
    "\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@jax.value_and_grad\n",
    "def loss(model: FullyConnectedWithTime, data: Array, key: PRNGKeyArray) -> Array:\n",
    "    key1, key2 = random.split(key, 2)\n",
    "\n",
    "    random_times = random.randint(key1, (data.shape[0],), minval=0, maxval=T)\n",
    "\n",
    "    # NOTE: noise will match as both use key2\n",
    "    noise = random.normal(key2, data.shape)\n",
    "    noised_data = forward_marginal(key2, data, random_times[:, jnp.newaxis])\n",
    "\n",
    "    # NOTE: rescale time to in [0, 1]\n",
    "    output = jax.vmap(model)(noised_data, random_times / (T - 1))\n",
    "\n",
    "    loss = jnp.mean((noise - output) ** 2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def single_loss_fn(model, data, t, key):\n",
    "    noise = random.normal(key, data.shape)\n",
    "    noised_data = forward_marginal(key, data, t)\n",
    "\n",
    "    output = model(noised_data, t / (T - 1))\n",
    "\n",
    "    return jnp.mean((noise - output) ** 2)\n",
    "\n",
    "\n",
    "def batch_loss_fn(model, data, key):\n",
    "    batch_size = data.shape[0]\n",
    "    t_key, loss_key = jr.split(key)\n",
    "    loss_key = jr.split(loss_key, batch_size)\n",
    "\n",
    "    # Low-discrepancy sampling over t to reduce variance\n",
    "    t = random.randint(t_key, (batch_size,), minval=0, maxval=T)\n",
    "\n",
    "    loss_fn = ft.partial(single_loss_fn, model)\n",
    "    loss_fn = jax.vmap(loss_fn)\n",
    "\n",
    "    return jnp.mean(loss_fn(data, t, loss_key))\n",
    "\n",
    "\n",
    "def dataloader(data, batch_size, *, key):\n",
    "    dataset_size = data.shape[0]\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        key, subkey = jr.split(key, 2)\n",
    "        perm = jr.permutation(subkey, indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield data[batch_perm]\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, data, key, opt_state, opt_update):\n",
    "    loss_fn = eqx.filter_value_and_grad(batch_loss_fn)\n",
    "    loss, grads = loss_fn(model, data, key)\n",
    "    updates, opt_state = opt_update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    key = jr.split(key, 1)[0]\n",
    "    return loss, model, key, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation hyperparameters\n",
    "num_steps = 40_000\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "print_every = 1_000\n",
    "\n",
    "model_key, train_key, loader_key, sample_key = jr.split(key, 4)\n",
    "\n",
    "data = task.normalize_x(train_x)\n",
    "val_data = task.normalize_x(val_x)\n",
    "\n",
    "model = FullyConnectedWithTime(data.shape[1], key=model_key)\n",
    "\n",
    "opt = optax.adabelief(lr)\n",
    "opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "total_value = 0\n",
    "total_size = 0\n",
    "for step, data_ in zip(range(num_steps), dataloader(data, batch_size, key=loader_key)):\n",
    "    value, model, train_key, opt_state = make_step(model, data_, train_key, opt_state, opt.update)\n",
    "    total_value += value.item()\n",
    "    total_size += 1\n",
    "    if (step % print_every) == 0 or step == num_steps - 1:\n",
    "        key, sub_key = jr.split(key)\n",
    "        val_loss = batch_loss_fn(model, val_data, sub_key)\n",
    "\n",
    "        print(\n",
    "            f\"Step={step:05}\",\n",
    "            f\"Train Loss={total_value / total_size:.4f}\",\n",
    "            f\"Val Loss={val_loss:.4f}\",\n",
    "            sep=\"\\t|\\t\",\n",
    "        )\n",
    "\n",
    "        total_value = 0\n",
    "        total_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DDIMVP(\n",
    "    num_steps=T,\n",
    "    shape=(100, train_x.shape[1]),\n",
    "    model=jax.vmap(model),  # assumes epsilon model (not score), so okay here!\n",
    "    beta_min=BETA_MIN,\n",
    "    beta_max=BETA_MAX,\n",
    "    eta=1.0,  # NOTE: equates to using DDPM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_base = sampler.sample(key)\n",
    "sample = task.denormalize_x(sample_base)\n",
    "sample = jnp.clip(sample, a_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SMCDiffOptOptimizer(base_sampler=sampler, gamma_t = lambda t: 1 - d_t(t))\n",
    "particle_samples = optimizer.optimize(key, lambda x: -task.predict(task.denormalize_x(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Load data\n",
    "data_path = Path().absolute() / \"design-bench\" / \"design_bench_data\" / \"superconductor\"\n",
    "\n",
    "x_files = sorted(data_path.glob(\"*x*.npy\"))\n",
    "y_files = sorted(data_path.glob(\"*y*.npy\"))\n",
    "\n",
    "x_data = jnp.vstack([jnp.load(file) for file in x_files])\n",
    "y_data = jnp.vstack([jnp.load(file) for file in y_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.mean(task.predict(task.denormalize_x(particle_samples)) / 185)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
